{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# I94 Database - Exploration Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\r\n",
    "import pandas as pd\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "\r\n",
    "path_immigration = \"output/fact/*\"\r\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\r\n",
    "df = spark.read.parquet(path_immigration)\r\n",
    "\r\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Enter the parameters you want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Enter Filter Parameters\r\n",
    "Year = 2016\r\n",
    "Month = 4\r\n",
    "MaxAge = 19\r\n",
    "Gender = \"F\"\r\n",
    "CountryOrigin = \"MALAYSIA\"\r\n",
    "State = \"FLORIDA\"\r\n",
    "City = \"Orlando\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-69668ce85763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m query = spark.sql(\"\"\"\n\u001b[0;32m      3\u001b[0m     \u001b[0mSELECT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mCOUNT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDISTINCT\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mFROM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\r\n",
    "\r\n",
    "query = spark.sql(\"\"\"\r\n",
    "    SELECT\r\n",
    "        year,\r\n",
    "        month,\r\n",
    "        state,\r\n",
    "        city,\r\n",
    "        origin_country,\r\n",
    "        gender,\r\n",
    "        age,\r\n",
    "        SUM(total_flights) AS total_flights,\r\n",
    "        SUM(total_people) AS total_people,\r\n",
    "        SUM(total_people)/SUM(total_flights) AS avg_per_flight\r\n",
    "    FROM\r\n",
    "        df\r\n",
    "    WHERE\r\n",
    "        year = {} AND\r\n",
    "        month = {} AND\r\n",
    "        age < {} AND\r\n",
    "        gender = {} AND\r\n",
    "        origin_country = {} AND\r\n",
    "        state = {} AND\r\n",
    "        city = {}\r\n",
    "    GROUP BY\r\n",
    "        year,\r\n",
    "        month,\r\n",
    "        state,\r\n",
    "        city,\r\n",
    "        origin_country,\r\n",
    "        gender,\r\n",
    "        age\r\n",
    "\"\"\".format(Year, Month, MaxAge, Gender, CountryOrigin, State, City))\r\n",
    "\r\n",
    "print(query.sort(query.total_people.desc()).show(5, truncate=True))\r\n",
    "\r\n",
    "pdf = query.toPandas()\r\n",
    "pdf.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "# df_spark.write.parquet(\"sas_data\")\n",
    "# df_spark=spark.read.parquet(\"sas_data\")\n",
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n",
    "#### Dimension Tables ####\n",
    "\n",
    "# Clean Airport Lookup\n",
    "\n",
    "print(\"      1) Airport Codes Dimension Table ETL started...\\n\")\n",
    "\n",
    "# Load raw Airport Code data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_airports)\n",
    "\n",
    "# Transform raw Airport Code dataset\n",
    "df.createOrReplaceTempView(\"df_airport\")\n",
    "df_airport = spark.sql('''\n",
    "        SELECT\n",
    "            iata_code as i94port,\n",
    "            municipality,\n",
    "            coordinates\n",
    "        FROM\n",
    "            df_airport\n",
    "        WHERE\n",
    "            iata_code IS NOT NULL OR iata_code != \"\"\n",
    "''')\n",
    "\n",
    "# Write cleaned Airport Codes data to S3 bucket as parquet files\n",
    "df_airport.write.parquet(output_data+\"dim/\",mode='overwrite')\n",
    "\n",
    "print(\"         Airport Codes ETL complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}